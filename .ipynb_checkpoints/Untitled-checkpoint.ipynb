{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f7c23f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23604\\2884196745.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d019f299",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23604\\1622384465.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msetuptools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind_packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtextrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m setup(\n\u001b[0;32m      5\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtextrank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textrank'"
     ]
    }
   ],
   "source": [
    "from setuptools import setup, find_packages\n",
    "import textrank\n",
    "\n",
    "setup(\n",
    "    name=textrank.__name__,\n",
    "    version=textrank.__version__,\n",
    "    url='https://github.com/lovit/textrank/',\n",
    "    author=textrank.__author__,\n",
    "    author_email='soy.lovit@gmail.com',\n",
    "    description='TextRank based Summarizer (Keyword and key-sentence extractor)',\n",
    "    packages=find_packages(),\n",
    "    long_description=open('README.md', encoding=\"utf-8\").read(),\n",
    "    zip_safe=False,\n",
    "    setup_requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "796d4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def scan_vocabulary(sents, tokenize, min_count=2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a0b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):\n",
    "    counter = defaultdict(int)\n",
    "    for s, tokens_i in enumerate(tokens):\n",
    "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
    "        n = len(vocabs)\n",
    "        for i, v in enumerate(vocabs):\n",
    "            if window <= 0:\n",
    "                b, e = 0, n\n",
    "            else:\n",
    "                b = max(0, i - window)\n",
    "                e = min(i + window, n)\n",
    "            for j in range(b, e):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                counter[(v, vocabs[j])] += 1\n",
    "                counter[(vocabs[j], v)] += 1\n",
    "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "    return dict_to_mat(counter, n_vocabs, n_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96f59aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a67a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence=2):\n",
    "    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    tokens = [tokenize(sent) for sent in sents]\n",
    "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
    "    return g, idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "081c9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "320ca0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_keyword(sents, tokenize, min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):\n",
    "    g, idx_to_vocab = word_graph(sents, tokenize, min_count, window, min_cooccurrence)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdc46e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):\n",
    "    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "\n",
    "    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)\n",
    "    return prod / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d12294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_keysentence(sents, tokenize, min_count, similarity, df=0.85, max_iter=30, topk=5):\n",
    "    g = sent_graph(sents, tokenize, min_count, min_sim, similarity)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keysents = [(idx, R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "    return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a4e46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "def komoran_tokenize(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e2697ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23604\\3400612071.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextrank\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeywordSummarizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m keyword_extractor = KeywordSummarizer(\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkomoran_tokenize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textrank'"
     ]
    }
   ],
   "source": [
    "from textrank import KeywordSummarizer\n",
    "\n",
    "keyword_extractor = KeywordSummarizer(\n",
    "    tokenize = komoran_tokenize,\n",
    "    window = -1,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "keywords = keyword_extractor.summarize(sents, topk=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0f5cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('gensim' in sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4635a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\90ldh\\AppData\\Roaming\\Python\\Python39\\site-packages\\gensim\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57ef97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\Untitled Folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b12e34f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\90ldh\\appdata\\roaming\\python\\python39\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\90ldh\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc87e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99f7388d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23604\\3256808454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextrank\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeywordSummarizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textrank'"
     ]
    }
   ],
   "source": [
    "from textrank import KeywordSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ca13830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textrankr\n",
      "  Downloading textrankr-1.1-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from textrankr) (2.8.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from textrankr) (63.4.1)\n",
      "Installing collected packages: textrankr\n",
      "Successfully installed textrankr-1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textrankr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173fe53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
